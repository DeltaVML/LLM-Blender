{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "data_dir = \"../../data\"\n",
    "dataset = \"mix_128\"\n",
    "set_name = \"test\"\n",
    "# post_fix = \"_filtered_with_chatgpt_with_reranker_ranks\"\n",
    "post_fix = \"\"\n",
    "data_file = f\"{data_dir}/{dataset}/{set_name}_data_prepared{post_fix}.json\"\n",
    "fuse_gen_data_dir = Path(f\"{data_dir}/{dataset}/fuse_gen/{set_name}/\")\n",
    "fuse_gen_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(data_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "def deduplicate_string(string, min_ngram=2, max_ngram=8, repeat=4):\n",
    "\n",
    "    result = \"\"\n",
    "    \n",
    "    sub_strings = string.split(\" \")\n",
    "    assert repeat >= 2, \"repeat should be larger than 2\"\n",
    "    for i in range(len(sub_strings)):\n",
    "        stop = False\n",
    "        for ngram in range(min_ngram, max_ngram):\n",
    "            current_ngrams = sub_strings[i:i+ngram]\n",
    "            # at least one alpha in the ngram\n",
    "            if not any([re.search(r\"[a-zA-Z]\", ngra) for ngra in current_ngrams]):\n",
    "                continue\n",
    "            if len(set([\" \".join(sub_strings[i+j*ngram:i+j*ngram+ngram]) for j in range(repeat)])) == 1:\n",
    "                stop = True\n",
    "                # keep the first occurrence\n",
    "                result += \" \" + \" \".join(sub_strings[i:i+ngram])\n",
    "                break\n",
    "        if stop:\n",
    "            break\n",
    "        else:\n",
    "            result += \" \" + sub_strings[i]\n",
    "    return result.strip()\n",
    "\n",
    "for item in tqdm(data):\n",
    "    for cand in item['candidates']:\n",
    "        cand['text'] = deduplicate_string(cand['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "random.seed(42)\n",
    "class GenFuserDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length, top_k, select_key, candidate_max_length=None, add_noise=False, noise_prob=0.10):\n",
    "        \"\"\"\n",
    "            data: list of dict\n",
    "            tokenizer: tokenizer\n",
    "            max_length: max length of the input sequence\n",
    "            top_k: number of top k candidate to select\n",
    "            select_key: selection metric for the top k candidates\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.top_k = top_k\n",
    "        self.select_key = select_key\n",
    "        self.candidate_max_length = candidate_max_length\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_prob = noise_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        intruction = item[\"instruction\"]\n",
    "        input = item[\"input\"]\n",
    "        output = item[\"output\"]\n",
    "        sorted_candidates = sorted(\n",
    "            item[\"candidates\"], \n",
    "            key=lambda x: x['scores'][self.select_key], reverse=True)\n",
    "        if self.top_k > 0:\n",
    "            top_k_candidates = sorted_candidates[:self.top_k]\n",
    "        else:\n",
    "            top_k_candidates = sorted_candidates\n",
    "        if self.add_noise and random.random() < self.noise_prob and len(sorted_candidates[self.top_k:]) > 0:\n",
    "            random_idx = random.randint(0, len(top_k_candidates)-1)\n",
    "            top_k_candidates[random_idx] = random.choice(sorted_candidates[self.top_k:])\n",
    "        random.shuffle(top_k_candidates)\n",
    "        candidates = [c[\"text\"] for c in top_k_candidates]\n",
    "        if self.candidate_max_length is not None:\n",
    "            for i in range(len(candidates)):\n",
    "                ids = self.tokenizer.encode(candidates[i], add_special_tokens=False)\n",
    "                if len(ids) > self.candidate_max_length:\n",
    "                    ids = ids[:self.candidate_max_length]\n",
    "                    candidates[i] = self.tokenizer.decode(ids)\n",
    "                    candidates[i] += \"...\"\n",
    "\n",
    "        # concatenate input and candidates\n",
    "        instruction = \"Instruction: \" + intruction # replace \"</s>\" with \"</s>\"\n",
    "        input = \"Input: \" + input\n",
    "        candidates = \"</s>\".join([f\"Candidate {i}: <extra_id_{i}>:\" + c for i, c in enumerate(candidates)]) # extra id\n",
    "        fuse_input = \"</s>\".join([instruction, input, candidates])\n",
    "        fuse_input += \"</s>Summarize candidates into a better one for the given instruction:\"\n",
    "\n",
    "        # tokenize\n",
    "        fuse_input_ids = self.tokenizer(\n",
    "            fuse_input,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False,)\n",
    "        labels_ids = self.tokenizer.encode(output, return_tensors=\"pt\", add_special_tokens=False,).squeeze(0)\n",
    "        fuse_input_ids = {k: v.squeeze(0) for k, v in fuse_input_ids.items()}\n",
    "\n",
    "\n",
    "        return {\n",
    "            **fuse_input_ids,\n",
    "            \"labels\": labels_ids,\n",
    "            \"source_models\": [c['model'] for c in top_k_candidates],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "random.seed(42)\n",
    "metric = \"bartscore\"\n",
    "add_noise = True\n",
    "noise_prob = 0.10\n",
    "save_postfix = \"_noise\" if add_noise else \"\"\n",
    "for top_k in [3, 5]:\n",
    "    dataset = GenFuserDataset(data, tokenizer, 1024, top_k, metric, 128, add_noise, noise_prob)\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "    gen_data = []\n",
    "    for item in tqdm(iter(data_loader), total=len(data_loader)):\n",
    "        gen_data.append(\n",
    "            {\n",
    "                \"input\": tokenizer.decode(item['input_ids'][0]),\n",
    "                \"output\": tokenizer.decode(item['labels'][0]),\n",
    "                \"source_models\": [x[0] for x in item['source_models']],\n",
    "            }\n",
    "        )\n",
    "    save_file = fuse_gen_data_dir / f\"top{top_k}_{metric}{save_postfix}.jsonl\"\n",
    "    with open(save_file, 'w') as f:\n",
    "        for item in gen_data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "    print(f\"Saved to {save_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "random.seed(42)\n",
    "for item in data:\n",
    "    for cand in item['candidates']:\n",
    "        cand['scores']['random'] = random.random()\n",
    "for top_k in [3, 5]:\n",
    "    dataset = GenFuserDataset(data, tokenizer, 1024, top_k, \"random\", 128)\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "    gen_data = []\n",
    "    for item in iter(data_loader):\n",
    "        gen_data.append(\n",
    "            {\n",
    "                \"input\": tokenizer.decode(item['input_ids'][0]),\n",
    "                \"output\": tokenizer.decode(item['labels'][0]),\n",
    "                \"source_models\": [x[0] for x in item['source_models']],\n",
    "            }\n",
    "        )\n",
    "    with open(fuse_gen_data_dir / f\"random{top_k}.jsonl\", 'w') as f:\n",
    "        for item in gen_data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "    print(f\"Saved to {fuse_gen_data_dir / f'random{top_k}.jsonl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "for item in data:\n",
    "    for cand in item['candidates']:\n",
    "        cand['scores']['random'] = random.random()\n",
    "        if \"oasst\" in cand['model']:\n",
    "            cand['scores']['random'] = 2.0 # make sure it is the best\n",
    "\n",
    "for top_k in [3]:\n",
    "    dataset = GenFuserDataset(data, tokenizer, 1024, top_k, \"random\", 128)\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "    gen_data = []\n",
    "    for item in iter(data_loader):\n",
    "        gen_data.append(\n",
    "            {\n",
    "                \"input\": tokenizer.decode(item['input_ids'][0]),\n",
    "                \"output\": tokenizer.decode(item['labels'][0]),\n",
    "                \"source_models\": [x[0] for x in item['source_models']],\n",
    "            }\n",
    "        )\n",
    "    with open(fuse_gen_data_dir / f\"oasst-random{top_k}.jsonl\", 'w') as f:\n",
    "        for item in gen_data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "    print(f\"Saved to {fuse_gen_data_dir / f'oasst-random{top_k}.jsonl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "for select_key in [\"SummaReranker\", \"deberta-random\", \"deberta-random-with-tgt\", \"deberta-top-bottom\", \"roberta-random\"]:\n",
    "    for top_k in [3, 5]:\n",
    "        dataset = GenFuserDataset(data, tokenizer, 1024, top_k, select_key, 128)\n",
    "\n",
    "        from torch.utils.data import DataLoader\n",
    "        data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "        gen_data = []\n",
    "        for item in tqdm(iter(data_loader), total=len(data_loader)):\n",
    "            gen_data.append(\n",
    "                {\n",
    "                    \"input\": tokenizer.decode(item['input_ids'][0]),\n",
    "                    \"output\": tokenizer.decode(item['labels'][0]),\n",
    "                    \"source_models\": [x[0] for x in item['source_models']],\n",
    "                }\n",
    "            )\n",
    "        with open(fuse_gen_data_dir / f\"top{top_k}_{select_key}.jsonl\", 'w') as f:\n",
    "            for item in gen_data:\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "        print(f\"Saved to {fuse_gen_data_dir / f'top{top_k}_{select_key}.jsonl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gpt_eval.utils import get_scores_from_cmps\n",
    "import random\n",
    "random.seed(42)\n",
    "train_predictions_path_map = {\n",
    "    \"deberta-bartscore\": \"./outputs/crosscompare/microsoft/deberta-v3-large/test_mix_128_train_PairReranker_full_comparison_bartscore/predictions_full.pt\"\n",
    "}\n",
    "train_predictions = {k: torch.load(v)[0] for k, v in train_predictions_path_map.items()}\n",
    "train_scores = {k: get_scores_from_cmps(v) for k, v in train_predictions.items()}\n",
    "for i, item in enumerate(data):\n",
    "    for j, cand in enumerate(item['candidates']):\n",
    "        for k, v in train_scores.items():\n",
    "            cand['scores'][k] = v[i][j]\n",
    "\n",
    "for select_key in train_scores.keys():\n",
    "    for top_k in [3, 5]:\n",
    "        dataset = GenFuserDataset(data, tokenizer, 1024, top_k, select_key, 128)\n",
    "\n",
    "        from torch.utils.data import DataLoader\n",
    "        data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "        gen_data = []\n",
    "        for item in tqdm(iter(data_loader), total=len(data_loader)):\n",
    "            gen_data.append(\n",
    "                {\n",
    "                    \"input\": tokenizer.decode(item['input_ids'][0]),\n",
    "                    \"output\": tokenizer.decode(item['labels'][0]),\n",
    "                    \"source_models\": [x[0] for x in item['source_models']],\n",
    "                }\n",
    "            )\n",
    "        with open(fuse_gen_data_dir / f\"top{top_k}_{select_key}.jsonl\", 'w') as f:\n",
    "            for item in gen_data:\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "        print(f\"Saved to {fuse_gen_data_dir / f'top{top_k}_{select_key}.jsonl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pair_ranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
