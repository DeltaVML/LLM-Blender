{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM-Blender Usage examples\n",
    "\n",
    "Please first download our DeBERTa-v3-large PairRanker checkpoint to your local folder: [checkpoint link](https://drive.google.com/drive/folders/1E3qsZqja5IBaYEDRtVARU88mDl_nBqQ3?usp=sharing)\n",
    "And put that to <your checkpoint path>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llm_blender\n",
    "ranker_config = llm_blender.RankerConfig\n",
    "ranker_config.ranker_type = \"pairranker\"\n",
    "ranker_config.model_type = \"deberta\"\n",
    "ranker_config.model_name = \"microsoft/deberta-v3-large\"\n",
    "ranker_config.load_checkpoint = \"<your checkpoint path>\"\n",
    "ranker_config.cache_dir = \"./hf_models\"\n",
    "ranker_config.source_max_length = 128\n",
    "ranker_config.candidate_max_length = 128\n",
    "ranker_config.n_tasks = 1\n",
    "fuser_config = llm_blender.GenFuserConfig\n",
    "fuser_config.model_name = \"llm-blender/gen_fuser_3b\"\n",
    "fuser_config.cache_dir = \"./hf_models\"\n",
    "fuser_config.max_length = 512\n",
    "fuser_config.candidate_max_length = 128\n",
    "blender_config = llm_blender.BlenderConfig\n",
    "blender_config.device = \"cuda\"\n",
    "blender = llm_blender.Blender(blender_config, ranker_config, fuser_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LLM-Blender for ranking\n",
    "By the rank function, LLM-Blender could ranks the candidates through pairwise comparisons and return the ranks. We show our ranker's ranks are highly correlated with ChatGPT ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 4/4 [00:43<00:00, 11.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from llm_blender.gpt_eval.cor_eval import COR_MAPS\n",
    "from llm_blender.gpt_eval.utils import get_ranks_from_chatgpt_cmps\n",
    "mixinstruct_test = datasets.load_dataset(\"llm-blender/mix-instruct\", split=\"test\", streaming=True)\n",
    "few_examples = list(mixinstruct_test.take(8))\n",
    "# remove cmp_results with none cmp results\n",
    "for ex in few_examples:\n",
    "    keys = list(ex['cmp_results'].keys())\n",
    "    for key in keys:\n",
    "        if not ex['cmp_results'][key]:\n",
    "            del ex['cmp_results'][key]\n",
    "few_examples = [x for x in few_examples if x['cmp_results']]\n",
    "inputs = [x['input'] for x in few_examples]\n",
    "candidates_texts = [[cand['text'] for cand in x['candidates']] for x in few_examples]\n",
    "ranks = blender.rank(inputs, candidates_texts, return_scores=False, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with ChatGPT\n",
      "------------------------\n",
      "corrcoef 0.502434644007648\n",
      "spearman 0.35554809046205055\n",
      "spearman_footrule 25.5\n",
      "set_based 0.6422190656565656\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "llm_ranks_map, gpt_cmp_results = get_ranks_from_chatgpt_cmps(few_examples)\n",
    "gpt_ranks = np.array(list(llm_ranks_map.values())).T\n",
    "print(\"Correlation with ChatGPT\")\n",
    "print(\"------------------------\")\n",
    "for cor_name, cor_func in COR_MAPS.items():\n",
    "    print(cor_name, cor_func(ranks, gpt_ranks))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LLM-Blender for fuse generation\n",
    "We show that the the fused generation using the top-ranked candidate from the rankers could get outputs of higher quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing candidates: 100%|██████████| 4/4 [00:12<00:00,  3.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from llm_blender.blender.blender_utils import get_topk_candidates_from_ranks\n",
    "topk_candidates = get_topk_candidates_from_ranks(ranks, candidates_texts, top_k=3)\n",
    "fuse_generations = blender.fuse(inputs, topk_candidates, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 4/4 [00:44<00:00, 11.06s/it]\n",
      "Fusing candidates: 100%|██████████| 4/4 [00:12<00:00,  3.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# # Or do rank and fuser together\n",
    "# fuse_generations, ranks = blender.rank_and_fuse(inputs, candidates_texts, return_scores=False, batch_size=2, top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bartscore: 100%|██████████| 8/8 [00:00<00:00, 41.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion Scores\n",
      "   bartscore: -3.8043667674064636\n",
      "LLM Scores\n",
      "0 oasst-sft-4-pythia-12b-epoch-3.5\n",
      "   bartscore: -3.807092547416687\n",
      "1 koala-7B-HF\n",
      "   bartscore: -4.550534904003143\n",
      "2 alpaca-native\n",
      "   bartscore: -4.206288725137711\n",
      "3 llama-7b-hf-baize-lora-bf16\n",
      "   bartscore: -3.9363586008548737\n",
      "4 flan-t5-xxl\n",
      "   bartscore: -4.934148460626602\n",
      "5 stablelm-tuned-alpha-7b\n",
      "   bartscore: -4.432858616113663\n",
      "6 vicuna-13b-1.1\n",
      "   bartscore: -4.20223930478096\n",
      "7 dolly-v2-12b\n",
      "   bartscore: -4.440025061368942\n",
      "8 moss-moon-003-sft\n",
      "   bartscore: -3.587637573480606\n",
      "9 chatglm-6b\n",
      "   bartscore: -3.7075400948524475\n",
      "10 mpt-7b\n",
      "   bartscore: -4.1352817714214325\n",
      "11 mpt-7b-instruct\n",
      "   bartscore: -4.282741814851761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llm_blender.common.evaluation import overall_eval\n",
    "metrics = ['bartscore']\n",
    "targets = [x['output'] for x in few_examples]\n",
    "scores = overall_eval(fuse_generations, targets, metrics)\n",
    "\n",
    "print(\"Fusion Scores\")\n",
    "for key, value in scores.items():\n",
    "    print(\"  \", key+\":\", np.mean(value))\n",
    "\n",
    "print(\"LLM Scores\")\n",
    "llms = [x['model'] for x in few_examples[0]['candidates']]\n",
    "llm_scores_map = {llm: {metric: [] for metric in metrics} for llm in llms}\n",
    "for ex in few_examples:\n",
    "    for cand in ex['candidates']:\n",
    "        for metric in metrics:\n",
    "            llm_scores_map[cand['model']][metric].append(cand['scores'][metric])\n",
    "for i, (llm, scores_map) in enumerate(llm_scores_map.items()):\n",
    "    print(f\"{i} {llm}\")\n",
    "    for metric, llm_scores in llm_scores_map[llm].items():\n",
    "        print(\"  \", metric+\":\", np.mean(llm_scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_reranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
